"""
Implementação de busca semântica com embeddings para documentos OCR
Suporte a múltiplos modelos de embedding e busca por similaridade
"""

import numpy as np
import sqlite3
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import os
from pathlib import Path
import asyncio
import pickle
import hashlib

# Imports condicionais para embeddings
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

@dataclass
class SemanticResult:
    """Resultado de busca semântica"""
    document_id: str
    chunk_id: int
    text_chunk: str
    similarity_score: float
    embedding_model: str
    created_at: datetime

class SemanticSearchEngine:
    """Motor de busca semântica para documentos OCR"""
    
    def __init__(self, db_path: str = None, model_name: str = None):
        self.db_path = db_path or Path.home() / \".claude\" / \"semantic_search.db\"\n        self.db_path.parent.mkdir(exist_ok=True)\n        \n        self.logger = logging.getLogger(__name__)\n        \n        # Configuração do modelo\n        self.model_name = model_name or \"sentence-transformers/all-MiniLM-L6-v2\"\n        self.embedding_model = None\n        self.embedding_dim = 384  # Dimensão padrão do modelo MiniLM\n        \n        # Cache de embeddings\n        self.embedding_cache = {}\n        \n        # Configurações\n        self.chunk_size = 512\n        self.chunk_overlap = 50\n        self.similarity_threshold = 0.6\n        \n        self.init_database()\n        self.load_model()\n    \n    def init_database(self):\n        \"\"\"Inicializa banco de dados para embeddings\"\"\"\n        try:\n            conn = sqlite3.connect(self.db_path)\n            cursor = conn.cursor()\n            \n            # Tabela de embeddings\n            cursor.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS semantic_embeddings (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    document_id TEXT NOT NULL,\n                    chunk_id INTEGER NOT NULL,\n                    text_chunk TEXT NOT NULL,\n                    chunk_hash TEXT NOT NULL,\n                    embedding BLOB NOT NULL,\n                    embedding_model TEXT NOT NULL,\n                    embedding_dim INTEGER NOT NULL,\n                    created_at TEXT NOT NULL,\n                    UNIQUE(document_id, chunk_id, chunk_hash)\n                )\n            \"\"\")\n            \n            # Tabela de configurações\n            cursor.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS semantic_config (\n                    key TEXT PRIMARY KEY,\n                    value TEXT NOT NULL,\n                    updated_at TEXT NOT NULL\n                )\n            \"\"\")\n            \n            # Índices para performance\n            cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_document_id ON semantic_embeddings(document_id)\")\n            cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_model ON semantic_embeddings(embedding_model)\")\n            cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_created_at ON semantic_embeddings(created_at)\")\n            \n            conn.commit()\n            conn.close()\n            \n            self.logger.info(\"Banco de dados semântico inicializado\")\n            \n        except Exception as e:\n            self.logger.error(f\"Erro ao inicializar banco semântico: {e}\")\n    \n    def load_model(self):\n        \"\"\"Carrega modelo de embedding\"\"\"\n        try:\n            if not SENTENCE_TRANSFORMERS_AVAILABLE:\n                self.logger.warning(\"sentence-transformers não disponível\")\n                return\n            \n            self.logger.info(f\"Carregando modelo: {self.model_name}\")\n            self.embedding_model = SentenceTransformer(self.model_name)\n            \n            # Determinar dimensão do embedding\n            test_embedding = self.embedding_model.encode([\"test\"])\n            self.embedding_dim = len(test_embedding[0])\n            \n            self.logger.info(f\"Modelo carregado. Dimensão: {self.embedding_dim}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Erro ao carregar modelo de embedding: {e}\")\n            self.embedding_model = None\n    \n    def split_text_into_chunks(self, text: str) -> List[str]:\n        \"\"\"Divide texto em chunks para embedding\"\"\"\n        if not text or len(text.strip()) < 10:\n            return []\n        \n        words = text.split()\n        chunks = []\n        current_chunk = []\n        current_length = 0\n        \n        for word in words:\n            word_length = len(word)\n            \n            # Se adicionar esta palavra exceder o tamanho do chunk\n            if current_length + word_length + 1 > self.chunk_size:\n                if current_chunk:\n                    chunks.append(' '.join(current_chunk))\n                    \n                    # Manter overlap\n                    overlap_words = current_chunk[-self.chunk_overlap:]\n                    current_chunk = overlap_words + [word]\n                    current_length = sum(len(w) for w in current_chunk) + len(current_chunk)\n                else:\n                    current_chunk = [word]\n                    current_length = word_length\n            else:\n                current_chunk.append(word)\n                current_length += word_length + 1\n        \n        # Adicionar último chunk\n        if current_chunk:\n            chunks.append(' '.join(current_chunk))\n        \n        return chunks\n    \n    def calculate_text_hash(self, text: str) -> str:\n        \"\"\"Calcula hash do texto para cache\"\"\"\n        return hashlib.md5(text.encode('utf-8')).hexdigest()\n    \n    async def create_embeddings(self, document_id: str, text: str) -> bool:\n        \"\"\"Cria embeddings para um documento\"\"\"\n        try:\n            if not self.embedding_model:\n                self.logger.warning(\"Modelo de embedding não disponível\")\n                return False\n            \n            # Dividir texto em chunks\n            chunks = self.split_text_into_chunks(text)\n            if not chunks:\n                self.logger.warning(f\"Nenhum chunk gerado para documento {document_id}\")\n                return False\n            \n            conn = sqlite3.connect(self.db_path)\n            cursor = conn.cursor()\n            \n            # Remover embeddings antigos\n            cursor.execute(\"DELETE FROM semantic_embeddings WHERE document_id = ?\", (document_id,))\n            \n            embeddings_created = 0\n            \n            for chunk_id, chunk_text in enumerate(chunks):\n                chunk_hash = self.calculate_text_hash(chunk_text)\n                \n                # Verificar se já existe embedding para este chunk\n                cursor.execute(\"\"\"\n                    SELECT id FROM semantic_embeddings \n                    WHERE document_id = ? AND chunk_id = ? AND chunk_hash = ?\n                \"\"\", (document_id, chunk_id, chunk_hash))\n                \n                if cursor.fetchone():\n                    continue  # Já existe\n                \n                # Criar embedding\n                embedding = self.embedding_model.encode([chunk_text])[0]\n                embedding_blob = pickle.dumps(embedding)\n                \n                # Salvar no banco\n                cursor.execute(\"\"\"\n                    INSERT INTO semantic_embeddings \n                    (document_id, chunk_id, text_chunk, chunk_hash, embedding, \n                     embedding_model, embedding_dim, created_at)\n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n                \"\"\", (\n                    document_id, chunk_id, chunk_text, chunk_hash, embedding_blob,\n                    self.model_name, self.embedding_dim, datetime.now().isoformat()\n                ))\n                \n                embeddings_created += 1\n            \n            conn.commit()\n            conn.close()\n            \n            self.logger.info(f\"Embeddings criados para documento {document_id}: {embeddings_created} chunks\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Erro ao criar embeddings: {e}\")\n            return False\n    \n    async def search_similar(self, query: str, max_results: int = 10, \n                           min_similarity: float = None) -> List[SemanticResult]:\n        \"\"\"Busca documentos similares usando embeddings\"\"\"\n        try:\n            if not self.embedding_model:\n                self.logger.warning(\"Modelo de embedding não disponível\")\n                return []\n            \n            # Criar embedding da query\n            query_embedding = self.embedding_model.encode([query])[0]\n            \n            # Buscar embeddings no banco\n            conn = sqlite3.connect(self.db_path)\n            cursor = conn.cursor()\n            \n            cursor.execute(\"\"\"\n                SELECT document_id, chunk_id, text_chunk, embedding, \n                       embedding_model, created_at\n                FROM semantic_embeddings\n                WHERE embedding_model = ?\n                ORDER BY created_at DESC\n            \"\"\", (self.model_name,))\n            \n            results = []\n            min_sim = min_similarity or self.similarity_threshold\n            \n            for row in cursor.fetchall():\n                doc_id, chunk_id, text_chunk, embedding_blob, model, created_at = row\n                \n                # Deserializar embedding\n                try:\n                    embedding = pickle.loads(embedding_blob)\n                except Exception as e:\n                    self.logger.warning(f\"Erro ao deserializar embedding: {e}\")\n                    continue\n                \n                # Calcular similaridade\n                if SKLEARN_AVAILABLE:\n                    similarity = cosine_similarity(\n                        [query_embedding], [embedding]\n                    )[0][0]\n                else:\n                    # Fallback para cálculo manual\n                    similarity = self.calculate_cosine_similarity(query_embedding, embedding)\n                \n                # Filtrar por similaridade mínima\n                if similarity >= min_sim:\n                    result = SemanticResult(\n                        document_id=doc_id,\n                        chunk_id=chunk_id,\n                        text_chunk=text_chunk,\n                        similarity_score=similarity,\n                        embedding_model=model,\n                        created_at=datetime.fromisoformat(created_at)\n                    )\n                    results.append(result)\n            \n            conn.close()\n            \n            # Ordenar por similaridade decrescente\n            results.sort(key=lambda x: x.similarity_score, reverse=True)\n            \n            return results[:max_results]\n            \n        except Exception as e:\n            self.logger.error(f\"Erro na busca semântica: {e}\")\n            return []\n    \n    def calculate_cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n        \"\"\"Calcula similaridade coseno manualmente\"\"\"\n        try:\n            dot_product = np.dot(vec1, vec2)\n            norm1 = np.linalg.norm(vec1)\n            norm2 = np.linalg.norm(vec2)\n            \n            if norm1 == 0 or norm2 == 0:\n                return 0.0\n            \n            return dot_product / (norm1 * norm2)\n            \n        except Exception as e:\n            self.logger.error(f\"Erro ao calcular similaridade: {e}\")\n            return 0.0\n    \n    async def get_document_embeddings_count(self, document_id: str) -> int:\n        \"\"\"Retorna número de embeddings para um documento\"\"\"\n        try:\n            conn = sqlite3.connect(self.db_path)\n            cursor = conn.cursor()\n            \n            cursor.execute(\n                \"SELECT COUNT(*) FROM semantic_embeddings WHERE document_id = ?\",\n                (document_id,)\n            )\n            \n            count = cursor.fetchone()[0]\n            conn.close()\n            \n            return count\n            \n        except Exception as e:\n            self.logger.error(f\"Erro ao contar embeddings: {e}\")\n            return 0\n    \n    async def delete_document_embeddings(self, document_id: str) -> bool:\n        \"\"\"Remove embeddings de um documento\"\"\"\n        try:\n            conn = sqlite3.connect(self.db_path)\n            cursor = conn.cursor()\n            \n            cursor.execute(\n                \"DELETE FROM semantic_embeddings WHERE document_id = ?\",\n                (document_id,)\n            )\n            \n            deleted_count = cursor.rowcount\n            conn.commit()\n            conn.close()\n            \n            self.logger.info(f\"Embeddings removidos para documento {document_id}: {deleted_count}\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Erro ao remover embeddings: {e}\")\n            return False\n    \n    async def clear_all_embeddings(self) -> bool:\n        \"\"\"Remove todos os embeddings\"\"\"\n        try:\n            conn = sqlite3.connect(self.db_path)\n            cursor = conn.cursor()\n            \n            cursor.execute(\"DELETE FROM semantic_embeddings\")\n            \n            deleted_count = cursor.rowcount\n            conn.commit()\n            conn.close()\n            \n            self.logger.info(f\"Todos os embeddings removidos: {deleted_count}\")\n            return True\n            \n        except Exception as e:\n            self.logger.error(f\"Erro ao limpar embeddings: {e}\")\n            return False\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatísticas dos embeddings\"\"\"\n        try:\n            conn = sqlite3.connect(self.db_path)\n            cursor = conn.cursor()\n            \n            # Estatísticas gerais\n            cursor.execute(\"SELECT COUNT(*) FROM semantic_embeddings\")\n            total_embeddings = cursor.fetchone()[0]\n            \n            cursor.execute(\"SELECT COUNT(DISTINCT document_id) FROM semantic_embeddings\")\n            total_documents = cursor.fetchone()[0]\n            \n            cursor.execute(\"SELECT COUNT(DISTINCT embedding_model) FROM semantic_embeddings\")\n            total_models = cursor.fetchone()[0]\n            \n            # Estatísticas por modelo\n            cursor.execute(\"\"\"\n                SELECT embedding_model, COUNT(*) as count, AVG(embedding_dim) as avg_dim\n                FROM semantic_embeddings\n                GROUP BY embedding_model\n            \"\"\")\n            \n            models_stats = {}\n            for row in cursor.fetchall():\n                model, count, avg_dim = row\n                models_stats[model] = {\n                    'count': count,\n                    'avg_dimension': int(avg_dim) if avg_dim else 0\n                }\n            \n            # Documentos mais recentes\n            cursor.execute(\"\"\"\n                SELECT document_id, MAX(created_at) as last_updated\n                FROM semantic_embeddings\n                GROUP BY document_id\n                ORDER BY last_updated DESC\n                LIMIT 5\n            \"\"\")\n            \n            recent_documents = []\n            for row in cursor.fetchall():\n                doc_id, last_updated = row\n                recent_documents.append({\n                    'document_id': doc_id,\n                    'last_updated': last_updated\n                })\n            \n            conn.close()\n            \n            return {\n                'total_embeddings': total_embeddings,\n                'total_documents': total_documents,\n                'total_models': total_models,\n                'current_model': self.model_name,\n                'embedding_dimension': self.embedding_dim,\n                'model_available': self.embedding_model is not None,\n                'models_stats': models_stats,\n                'recent_documents': recent_documents,\n                'chunk_size': self.chunk_size,\n                'similarity_threshold': self.similarity_threshold\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"Erro ao obter estatísticas: {e}\")\n            return {}\n    \n    def set_similarity_threshold(self, threshold: float):\n        \"\"\"Define threshold de similaridade\"\"\"\n        if 0.0 <= threshold <= 1.0:\n            self.similarity_threshold = threshold\n            self.logger.info(f\"Threshold de similaridade definido para: {threshold}\")\n        else:\n            self.logger.warning(f\"Threshold inválido: {threshold}. Deve estar entre 0.0 e 1.0\")\n    \n    def set_chunk_size(self, size: int):\n        \"\"\"Define tamanho dos chunks\"\"\"\n        if size > 0:\n            self.chunk_size = size\n            self.logger.info(f\"Tamanho do chunk definido para: {size}\")\n        else:\n            self.logger.warning(f\"Tamanho de chunk inválido: {size}\")\n    \n    def is_available(self) -> bool:\n        \"\"\"Verifica se busca semântica está disponível\"\"\"\n        return (SENTENCE_TRANSFORMERS_AVAILABLE and \n                self.embedding_model is not None)\n    \n    def get_requirements(self) -> List[str]:\n        \"\"\"Retorna lista de dependências necessárias\"\"\"\n        requirements = []\n        \n        if not SENTENCE_TRANSFORMERS_AVAILABLE:\n            requirements.append(\"sentence-transformers\")\n        \n        if not SKLEARN_AVAILABLE:\n            requirements.append(\"scikit-learn\")\n        \n        if not OPENAI_AVAILABLE:\n            requirements.append(\"openai\")\n        \n        return requirements\n\n# Função de conveniência para criar instância\ndef create_semantic_search_engine(model_name: str = None) -> SemanticSearchEngine:\n    \"\"\"Cria instância do motor de busca semântica\"\"\"\n    return SemanticSearchEngine(model_name=model_name)